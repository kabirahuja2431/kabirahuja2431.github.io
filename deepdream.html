<!DOCTYPE html>
<html lang="en">

  <head>

    <!-- Meta Tag -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <!-- SEO -->
    <meta name="description" content="150 words">
    <meta name="author" content="uipasta">
    <meta name="url" content="http://www.yourdomainname.com">
    <meta name="copyright" content="company name">
    <meta name="robots" content="index,follow">


    <title>Deep Dream</title>

    <!-- Favicon -->
    <link rel="shortcut icon" href="images/favicon/favicon.ico">
    <link rel="apple-touch-icon" sizes="144x144" type="image/x-icon" href="images/favicon/apple-touch-icon.png">

    <!-- All CSS Plugins -->
    <link rel="stylesheet" type="text/css" href="css/plugin.css">

    <!-- Main CSS Stylesheet -->
    <link rel="stylesheet" type="text/css" href="css/style.css">

    <!-- Google Web Fonts  -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Poppins:400,300,500,600,700">

    <!-- Syntax Highlighter  -->
    <link rel="stylesheet" type="text/css" href="css/syntax/shCore.css">
    <link rel="stylesheet" type="text/css" href="css/syntax/shThemeDefault.css">

    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <!-- HTML5 shiv and Respond.js support IE8 or Older for HTML5 elements and media queries -->
    <!--[if lt IE 9]>
	   <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
	   <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->


  </head>

 <body>



	 <!-- Preloader Start -->
     <div class="preloader">
	   <div class="rounder"></div>
      </div>
      <!-- Preloader End -->




    <div id="main">
        <div class="container">
            <div class="row">



                 <!-- About Me (Left Sidebar) Start -->
                 <div class="col-md-3">
                   <div class="about-fixed">

                     <div class="my-pic">
                        <img src="images/kab_4c.jpg" alt="">
                        </div>



                      <div class="my-detail">

                        <div class="white-spacing">
                            <h1>Kabir Ahuja</h1>
                            <span>Deep Learning Enthusiast</span>
                        </div>

                       <ul class="social-icon">
                         <li><a href="https://www.facebook.com/kabirahuja31" target="_blank" class="facebook"><i class="fa fa-facebook"></i></a></li>
                         <li><a href="https://www.linkedin.com/in/kabir-ahuja-aa1942123/" target="_blank" class="linkedin"><i class="fa fa-linkedin"></i></a></li>
                         <li><a href="https://github.com/kabirahuja2431" target="_blank" class="github"><i class="fa fa-github"></i></a></li>
                        </ul>

                    </div>
                  </div>
                </div>
                <!-- About Me (Left Sidebar) End -->





                 <!-- Blog Post (Right Sidebar) Start -->
                 <div class="col-md-9">
                    <div class="col-md-12 page-body">
                    	<div class="row">


                            <div class="sub-title">
                           		<a href="index.html" title="Go to Home Page"><h2>Back Home</h2></a>
                                <a href="#comment" class="smoth-scroll"><i class="icon-bubbles"></i></a>
                             </div>


                            <div class="col-md-12 content-page">
                              <div class="col-md-12 blog-post">


                                <!-- Post Headline Start -->
                                <div class="post-title">
                                    <h1>Dreaming with Neural Nets: Deep Dream</h1>
                                   </div>
                                   <!-- Post Headline End -->


                                <!-- Post Detail Start -->
                                <div class="post-info">
                                    <span>June 30, 2017 / by <a href="#" target="_blank">Kabir Ahuja</a></span>
                                   </div>
                                   <!-- Post Detail End -->


                                    <p>Hi guys, as promised in my last blog, today we will discuss and implement the very famous <b>'Deep Dream' </b>. Deep dream is a very cool application of Convolutional Neural Networks, in which we feed an image to a pretrained conv net and the convnet dreams on the image. So lets see some images to get a better intuition.

                                      <div class="row margin-top-40 margin-bottom-40">

                                        <div class="col-md-6 col-sm-6 col-xs-12">
                                          <h4>Original Image</h4>
                                          <img src="images/DeepDream/jump-1822412_1280.jpg" class="img-responsive" alt="content_image">

                                        </div>

                                        <div class="col-md-6 col-sm-6 col-xs-12">
                                            <h4>Dreamed Image </h4>
                                          <img src="images/DeepDream/jumpdream4c.jpg" class="img-responsive" alt="style_image">

                                        </div>


                                     </div>

                                     <div class="row margin-top-40 margin-bottom-40">

                                       <div class="col-md-6 col-sm-6 col-xs-12">
                                         <h4>Original Image </h4>
                                         <img src="images/DeepDream/mountain-2143877_640.jpg" class="img-responsive" alt="content_image">

                                       </div>

                                       <div class="col-md-6 col-sm-6 col-xs-12">
                                         <h4>Dreamed Image </h4>
                                         <img src="images/DeepDream/mountain_dream.jpg" class="img-responsive" alt="style_image" >

                                       </div>

                                    </div>
                                   <p>So as you can see there are no dogs or some other weird things in the original image, but we can see those things in the dreamed image. So lets discuss about it in quite a bit of detail.</p>
                                   <h2> Why Deep Dream </h2>
                                   <p>Before we understand the theory behind deep dream and its implementation, lets first see why are we doing it in the first place. No doubt it produces some very cool(sometimes creepy too :p) images, but is that it? The answer is obviously no.
                                     So, the Conv nets have become immensely popular for tasks involving image classification, object recognition, localisation etc etc, the accuracy produced by conv nets is indeed commendable, some of the deep
                                     conv net architecures can give around 95% accuracy on Image net(Image net is dataset of 1000 kinds of images, and the job of the conv net is to classify them). We know that the conv nets perform extremely well on the images, but a very
                                     little is known about how they are able to perform so well. Its easy to explain by saying, that in a conv net we have filters which look for particular features in images. But how do we prove that its indeed true or have a sort of intuition
                                     of what these filters are doing. Deep Dream does exactly that, it helps us to visualise how the conv nets actually work. As we saw in the above images, we were able to produce images with dogs in them without having dogs in the original images,
                                     So how it works is, we take a pre trained conv net which was trained for an image classification task and when we feed our image to it, it tries to look for the kinds of features it has looked in the images it was trained on. Lets see a more concrete
                                     explanation.

                                   </p>

                                  <h2>How Deep Dream Works</h2>
                                  <p>
                                    Lets break it down how the deep dream works. We will take a pretrained conv net as i mentioned earlier. As we know that a conv net is basically a collection of different types of layers(conv layers, pool layers, dropouts etc),
                                    and these layers represent the different activations of the input, and the activations are produced by convolving the input with different filters, so if a filter looks for dog like features in the input, it will give a higher value of
                                    activation if it finds such features in the input. So what we will do is feed our image to the conv net, and take the activation at a particular layer and we will try to maximize that activation. Lets say we have a layer 'l' in our conv net
                                    which is produced by a filter looking for dog like features in the image, so when i feed my image to the net and check the activation at that layer, i will get some value for the activation, so now what i will do is start making changes in the image
                                    so that i maximize that activation, just like we change parameters while training a neural network to minimize the loss function, this time instead we area changing our input to maximize the activation at a particular layer.
                                    Like we use gradient descent to minimize the loss function while training, here we will use gradient ascent(same as the gradient descent, only difference being we will add the gradients to parameters which in our case is the image, while updating them)
                                    on our image to maximize the activation scores at a particular layer.The update equation can be written as:<br><br>
                                    <font size="6">I = I + learning_rate*grad<br></font>
                                    <br>
                                    where I is our image and grad is the gradient of the activation score at a layer(which we want to maximize) with respect to the Image.
                                    And thats pretty much it, concept behind deep dream is really simple and very intuitive and now we are ready to see its implementation.
                                  </p>
                                  <h2>Getting the pre trained Convnet</h2>
                                  <p>
                                    As I have already mentioned like a hundred times now, in deep dream we need a pre trained conv net. For the purpose of this tutorial we will be using Google's Inception net which was winner of Image net challange in 2015(maybe :/).
                                    So First we will download the model, and then construct a computational graph in tensor flow from the model. Lets see the code, then i'll explain it in detail.

                                    <div class="margin-top-40 margin-bottom-40">
                                     <pre class="brush: js">

                                       #downloading Inception Net
                                       url = ('https://storage.googleapis.com/download.tensorflow.org/models/'
                                                  'inception5h.zip')
                                       basename = 'inception5h.zip'
                                       data_dir = ''
                                       def download_model():
                                           local_zip_file = os.path.join(data_dir,basename)
                                           if not os.path.exists(local_zip_file):
                                               model_url = urllib.request.urlopen(url)
                                               with open(local_zip_file) as output:
                                                   output.write(model_url.read())
                                           #Extracting the downloaded
                                           #zip file of the model.
                                           print('Extracting',local_zip_file)
                                           zip_ref = zipfile.ZipFile(local_zip_file,'r')
                                           zip_ref.extractall(data_dir)
                                           zip_ref.close()

                                      #Constructing computational graph
                                      #from downloaded model

                                      #the name of the proto buffer file
                                      #of the inception model
                                      model_fn = 'tensorflow_inception_graph.pb'
                                      graph = tf.Graph()
                                      #Starting an Interactive Session
                                      sess = tf.InteractiveSession(graph = graph)
                                      #opening the protobuffer
                                      #file containing the graph.
                                      with tf.gfile.FastGFile(os.path.join(data_dir,model_fn),'rb') as f:
                                        #graph_def will contain our graph object
                                        #this statement creates an empty graphdef object
                                        graph_def = tf.GraphDef()
                                        #Reading from the pb file
                                        #and parsing the graph object to graph_def
                                        graph_def.ParseFromString(f.read())

                                      #placeholder for input_image
                                      input_image = tf.placeholder(tf.float32,name='input_image')
                                      #the mean of the images on which
                                      #inception was trained on
                                      image_net_mean = 117.0
                                      #adding an extra dimension,since inception
                                      #net expects the input in this form
                                      #Also subtracting the imagenet mean from the image
                                      image_prepro = tf.expand_dims(input_image - image_net_mean,0)
                                      #Importing the graph from graph_def
                                      tf.import_graph_def(graph_def, {'input':image_prepro})
                                      #Storing the name of different layers of
                                      #inception net in layers list
                                      #we are only storing the names of Convolutional layers.
                                      layers = [op.name for op in graph.get_operations() if op.type=='Conv2D' and 'import/' in op.name]
                                    </pre>
                                  </div>
                                 </p>
                                 <p>
                                   So in the first few lines of the code we downloaded the inception net using the <b>urllib</b> module of python and extracted the contents using the <b>ZipFile</b> module.
                                   The extracted files will contain a file named <b>'tensorflow_inception_graph.pb'</b>, pb is basically a <b>protocol buffer</b> file. A protocol buffer is a text file which conatins
                                   the data structures of our model and generates classes and objects in programming languages like python, C when loaded. So the 'tensorflow_inception_graph.pb' file will
                                   be basically used to load the pretrained inception net model. <b>'gfile.FastGFile'</b> is just a method of tensor flow for opening files. The <b>graph_def</b> object contains the computational
                                   graph, so what we do is create an empty graph_def object first and then read the pb file and parse it to the graph_def object using <b>ParseFromString</b> method. Hence we obtain
                                   our computational graph in graph_def object and then we load the graph into the tensorflow session using <b>import_graph_def</b> method of tensor flow. Finally we store the name of
                                   different layers of the conv net in a ist called <b>'layers'</b>. Lets print the list and see what we get.<br></p>
                                    <img src="images/DeepDream/layers.png">
                                    <br>
                                   <p>
                                    Now we have our full computational graph
                                   of the model, and now we can easily calculate the activations at different layers as well as can optimize various parameters which we will see soon.
                                 </p>
                                 <!-- Post Coding (SyntaxHighlighter) Start -->
                                <!-- Post Coding (SyntaxHighlighter) End -->
                                <h2>Getting the gradients</h2>
                                <p>
                                  As discussed above, to update our image first we must calculate the gradients of the activation scores of the layer with respect to the image. Now the problem is, calculating gradients with respect to very
                                  large images will eat up a lot of RAM, to avoid this what we do is calculate the gradients with respect to the small portions(called tiles) of the image at a time. Lets see the code:
                                </p>
                                  <!-- Post Coding (SyntaxHighlighter) Start -->
                                  <div class="margin-top-40 margin-bottom-40">
                                   <pre class="brush: js">

                                     #tiled_gradients

                                     #this function returns the actual
                                     #size of the tiles in which we can
                                     #divide our image
                                     def get_tile_size(num_pixels,tile_size=512):
                                         num_tiles = int(round(num_pixels/tile_size))
                                         num_tiles = max(1,num_tiles)
                                         actual_tile_size = int(num_pixels/num_tiles)
                                         return actual_tile_size


                                     #Since it would be computationally expensive
                                     #to calculate the gradient with respect to the
                                     #full image, instead we will calculate the gradients
                                     #small portions(tiles) of the image.
                                     def tiled_gradients(img,gradients,tile_size=512):
                                         '''
                                         inputs:
                                             img: The input image
                                             gradients: A gradient node which holds the
                                                 the operations to find the gradient w.r.t
                                                 some input
                                             tile_size: smallest portion of the image
                                                 of which we want to calculate the gradient
                                                 at a time
                                         '''
                                         #getting the height and width of the image
                                         w,h = img.shape[:2]

                                         #tile size along x axis
                                         x_tile_size = get_tile_size(w,tile_size)
                                         #tile size along y axis
                                         y_tile_size = get_tile_size(h,tile_size)
                                         x_start = 0
                                         grad = np.zeros_like(img)
                                         while(x_start < w):
                                             #last x cordinate of the tile
                                             x_end = x_start + x_tile_size
                                             #shouldnt be greater than the width
                                             #of the image
                                             x_end = min(w,x_end)
                                             y_start = 0
                                             while(y_start < h):
                                                 y_end = y_start + y_tile_size
                                                 y_end = min(h,y_end)
                                                 #tile from the image
                                                 img_tile = img[x_start:x_end,y_start:y_end,:]
                                                 #calculating gradients w.r.t the tile of the image
                                                 g = sess.run(gradients,{input_image : img_tile})
                                                 #normalising gradients
                                                 g /= (np.std(g) + 1e-8)
                                                 grad[x_start:x_end,y_start:y_end,:] = g
                                                 #moving to the next tile
                                                 y_start = y_end
                                             #next tile
                                             x_start = x_end
                                         return grad

                                  </pre>
                                 </div>
                                 <!-- Post Coding (SyntaxHighlighter) End -->
                                 <p>
                                   The first method <b>'get_tile_size'</b> takes the input as num_pixels(height or width) and tile size and returns the actual tile size according to the pixels. Lets say we have an image of 1000x1000 px
                                   and we have given the tile size as 512, it is not possible to divide a 1000x1000 image to 512x512 px tiles, since the size of the image can be anything, this function gives us the actual size
                                   of the tile nearest to the size given by us in which we can divide our image.<br>
                                   In the second method <b>'tiled gradients'</b> we calculate the gradients. It takes as input, the image and the gradients which is a tensor that holds operations necessary to calculate the gradient.
                                   After that we using the get_tile_size method we find the size of the tiles in both x and y direction. Then we iterate in a loop considering all the tiles of the image and find the gradient with respect to each of them.
                                   If the <b>'gradients'</b> tensor is confusing you, dont worry it will become clear once we discuss the <b>'train'</b> function
                                  </p>
                                    <h2>Let the Training Begin</h2>
                                    <p>Now that we have our pre trained model as well as a method to find the gradients, we can start the training process. So what we will do during training is, feed the image to the convnet, find activations at a given layer and then
                                      calculate the score by taking the mean of these activations. Now our purpose in life is to maximize this score. We will use the gradients of the score with respect to the image to update it using gradient ascent, carry out this process for some
                                      iterations and finally we will get our Dreamed Image.<br>
                                      But there is a slight complication in this process. The Inception net was trained on images of size 224x224x3 pixels, so if we feed higher resolution images, it will create very fine features on the image.
                                      To address this issue what we do is, down scale the image a number of times to get a smaller resolution image, then deep dream on that image, upscale the dreamed image, recombine the dreamed image with previous higher res image, deep dream again and repeat this process
                                      till we have a dreamed image of the resolution of the original image. Below is a flow chart explaining this process.

                                    </p>
                                    <div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile userAgent=\&quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\&quot; version=\&quot;6.8.9\&quot; editor=\&quot;www.draw.io\&quot; type=\&quot;device\&quot;&gt;&lt;diagram id=\&quot;b45817b8-e237-a939-c1ef-92aa372b9edb\&quot; name=\&quot;Page-1\&quot;&gt;7Vpbb9sgFP41eWxlc3Hsx962PWzSpE7a9khj4rA5xnPIbb9+YEMcjNtVa0xaa5GSwAFz8MfHOYdjT+DNcve+IuXiE09pPgFBupvA2wkA8RTIXyXYNwKIokaQVSxtRGEruGe/qRYGWrpmKV1ZHQXnuWClLZzxoqAzYclIVfGt3W3Oc1trSTLqCO5nJHelX1kqFloaRknb8IGybKFVx2DaNDyQ2c+s4utC65sAOK8/TfOSmLH0ja4WJOXbIxG8m8CbinPRlJa7G5oraA1szXXvHmk9zLuihXjOBXraK7E3t05TiYSu8koseMYLkt+10uv69qgaIJC1hVjmshjK4g8qxF4vJlkLLkXtCB85L3W/RqdS9OistWjF19VM94KaB6TKqIH9gJgkIuVLKqq97FLRnAi2sUcnmhLZod/h0s+cSb0g0OyFsV5lTd4wDuwhminoq1pwZeFoGq2ohrwffgTfNP4weeECWNA9gZPWvSH5mpqNFeVyFtdzXi9dC2D0a81Nw8WqBuNKdgjjctc2ylKm/sMgCHbqxwwnJ9KM2LQbcco2XZFRbGyXGRq6WthSmZunNUixpaTDiu2CCXpfknoxttLm2itPVmVjBudsp5ihB9jQStDd04vsrp65IOlsA2OztkcW0cgWR8bwIHzJihtPcJqdIW+52n/T8rryXVUu8eC7JnZ3jbdNEw0DYY1aC+I5IATYE4bxQIYHS7uD/5udHrODgo7ZQa7ZiXusTtdJ/5PROak7Pt+OSdwdEwJPOyZxdgzAwU5+e0jdZegroyKedqiIsUNF3ENFfAIqwtBB4BRUDCee/V8Izmi9jfI+Mr4+eoHEH71C5CADL2X9ltJS/VWULB2M5K0KG5aKSkdEHuoOinKlOhPV08LXE3yrgJOM0s6qxjFnWSHLOZ2roRR+TB67r7RYKLZdr+QysCL7UlPvAp0G7SiY2mjDnnA26IH7JNEsduAOFdzy7F+s6rTDqLCGoGM4IxfrZCioI3fPjxhqHMC/+qihoAauEcEK6hlfPrBiZEAjjGz70ROXTgcCGp40TXe+uBTgnmDgpUm854KIvByHh8/D9YHoC0Pg+jE/5+HzR2vdc+nhwOnhXIpGchjoS+V4yyGfLZfz+riLelK5Q3EXumcwpIKEdTnKaCy2jUQC/AUJLr+jcR/pELZjXxi7aA92pEN4HOEE7EkWQl8hWeIDw+Hj2j4Mka+YzCg/w8PR83u27kNKFPRkdQZ7SIlc3zYdrW+DsAM1RN58G5o6QMejzTQAbAMNAzelMxjQQwXJft60mDNpztXtvMlHn11jFoU+jZl5rHC09Mm440cQ2XhjY2YGiB9ltX2rr3lTrH1zEt79AQ==&lt;/diagram&gt;&lt;/mxfile&gt;&quot;}"></div>
                                    <script type="text/javascript" src="https://www.draw.io/js/viewer.min.js"></script>
                                    <br>
                                    <p>
                                      Lets see this whole process in code. First we define some helper functions for training and the finally define the train function.
                                    </p>
                                    <div class="margin-top-40 margin-bottom-40">
                                     <pre class="brush: js">

                                       #some helper functions

                                       #A method to show the
                                       #image using matplotlib
                                       def show_image(img):
                                           img = np.uint8(np.clip(img, 0, 1)*255)
                                           plt.imshow(img)
                                           plt.show()


                                       #A method to save
                                       #the image on the disk
                                       def save_image(img,filename):
                                           img = np.uint8(np.clip(img, 0, 1)*255)
                                           pil_image = PIL.Image.fromarray(img)
                                           pil_image.save(filename)


                                       #Resizing the image
                                       #helpful while training
                                       def resize(img,size):
                                           #First we will convert numpy image
                                           #to PIL image, convention for
                                           #height and width is opposite
                                           #in PIL and numpy hence we
                                           #reverse the size.
                                           size = tuple(reversed(size))
                                           img = np.clip(img,0.0,255.0)
                                           img = img.astype(np.uint8)
                                           img = PIL.Image.fromarray(img)
                                           img = img.resize(size, PIL.Image.LANCZOS)
                                           img = np.float32(img)
                                           return img                                    </pre>
                                   </div>

                                 <div class="margin-top-40 margin-bottom-40">
                                  <pre class="brush: js">
                                    def train(layer_act,img0,num_iter=10, lr=1.5,num_octaves=4,scale = 1.4,show_n_save=True):

                                    '''
                                    Inputs:
                                    layer_act: Activation of the input image at some layer.
                                    img0 : Input image
                                    num_iterations: number of iterations for training
                                    num_octaves: number of times we will downscale the image
                                    scale: amount by which we will downscale the image
                                    show_n_save: whether to plot and save the image or not.
                                    '''

                                    #getting the score from the activations
                                    #by calculating the mean
                                    score = tf.reduce_mean(layer_act)

                                    #defining the gradient of the score w.r.t image
                                    gradients = tf.gradients(score,input_image)[0]
                                    img = img0.copy()

                                    #this list will contain different resolution copies
                                    #of our input image
                                    octaves = []
                                    for _ in range(num_octaves-1):
                                        size = img.shape[:2]
                                        #downscaling the image
                                        img_res = resize(img,np.int32(np.float32(size)/scale))
                                        #upscaling the downscaled image
                                        #and subtracting it from the current
                                        #image, so that we can late on do
                                        #the recombination.
                                        img_high = img - resize(img_res,size)
                                        octaves.append(img_high)
                                        img = img_res

                                    for i in range(num_octaves):
                                      if i > 0:
                                        img_high = octaves[-i]
                                        #combining the downscaled image with
                                        #the upscaled image.
                                        img = resize(img,img_high.shape[:2]) + img_high

                                      #Deep dreaming on the image
                                      for _ in range(num_iter):
                                        grad = tiled_gradients(img,gradients)
                                        #updating the image
                                        img += grad*(lr / (np.abs(grad).mean()+1e-7))

                                      if show_n_save:
                                        show_image(img/255)
                                        save_image(img/255,str(i)+'.jpg')
                                    return img
                                 </pre>
                                </div>
                                <p>
                                  First we calculate the scores from activations, then downscale the image num_octaves times and store the current image - upscaled downscaled image to the list octaves,
                                  which we will use while recombining. Let me simplify what i just said, as we saw in the flow chart in order to get good features on our image we downscale the image a number
                                  of times, deep dream on them and  then upscale them, So a doubt must have come across your mind that upscaling we will lead to loss in quality of the image, hence we maintain a list
                                  of one step higher resolution image for every image(expect the original size image) in a list, so that we can use them while recombining and avoid any loss in quality.
                                </p>
                                <p>
                                  Then for different octaves we deep dream on the image. If its the first octave, we simply perform deep dream on it, and in subsequent octaves we first perform the recombination
                                  and then deep dream on that image. You must still be confused about how did we get the scores at a particular layer, lets see that now:
                                </p>
                                <div class="margin-top-40 margin-bottom-40">
                                 <pre class="brush: js">

                                   #opening the image we want to
                                   #deep dream on
                                   img0 = PIL.Image.open('kab.jpg')
                                   #converting it into a numpy ndarray
                                   img0 = np.float32(img0)
                                   #selecting the layer at which we
                                   #we will calculate the activations
                                   layer = 'mixed4c'
                                   #getting the activation tensor for that layer
                                   layer_act = graph.get_tensor_by_name("import/%s:0"%layer)
                                   #finally calling the train function
                                   train(t_obj,img0,20)                            </pre>
                               </div>
                               <p>
                                 We can try different layers, feel free to experiment. Here are some images generated by different layers.
                               </p>
                               <div class="row margin-top-40 margin-bottom-40">

                                 <div class="col-md-12 col-sm-6 col-xs-12">
                                   <h4>Original Image</h4>
                                   <img src="images/DeepDream/aashima.jpg" class="img-responsive" alt="content_image">

                                 </div>

                              </div>
                              <div class="row margin-top-40 margin-bottom-40">
                              <div class="col-md-12 col-sm-6 col-xs-12">
                                  <h4>Mixed4c Layer</h4>
                                <img src="images/DeepDream/4c.jpg" class="img-responsive" alt="style_image">

                              </div>
                            </div>
                            <div class="row margin-top-40 margin-bottom-40">
                            <div class="col-md-12 col-sm-6 col-xs-12">
                                <h4>Mixed4e Layer</h4>
                              <img src="images/DeepDream/4e.jpg" class="img-responsive" alt="style_image">

                            </div>
                          </div>
                          <div class="row margin-top-40 margin-bottom-40">
                          <div class="col-md-12 col-sm-6 col-xs-12">
                              <h4>Mixed4a Layer</h4>
                            <img src="images/DeepDream/4a.jpg" class="img-responsive" alt="style_image">

                          </div>
                        </div>

                        <h2>Implementing Deep Dream on Videos</h2>
                        <p>
                          Now we know how to implement deep dream on images, lets extend it to videos. We will follow the exact same steps for the videos too, all we need to do is take all frames in the video,
                          deep dream on every one of them and then recombine all the frames to a video. Lets see how it looks in the code.

                        </p>
                        <div class="margin-top-40 margin-bottom-40">
                         <pre class="brush: js">

                           filename = 'flash.mp4'
                           #reader object for reading the video
                           #using imageio module for video processing
                           reader = imageio.get_reader(filename,'ffmpeg')
                           #getting number of frames and frames per second
                           fps = reader.get_meta_data()['fps']
                           nframes = reader.get_meta_data()['nframes']
                           img = []
                           #reading frames from video one by one
                           #and applying deep dream on them
                           for i in range(int(nframes-fps*10)):
                               layer = 'mixed4c'
                               t_obj = graph.get_tensor_by_name("import/%s:0"%layer)
                               img.append(train(t_obj,reader.get_data(i),10,num_octaves=3,shownsave=False))
                               print('done'+str(i))

                           #writing the dreamed frames into a video
                           writer = imageio.get_writer('dream','ffmpeg',fps = fps)
                           for i in range(len(img)):
                               writer.append_data(img[i])
                           writer.close()
                           reader.close()                        </pre>
                       </div>
                       <p>
                         So what we did here is read the video using <b>'get_reader'</b> method of imageio, i am using python's library imageio, you can use openCV or scikit video too.
                         Then we read every frame from the video and apply deep dream on that and finally write all the dreamed frames into a video.<br>
                         So thats it guys, i hope now you have a very good idea about how deep dream works and how to implement it. You can check the <a href="https://github.com/kabirahuja2431/My-projects/tree/master/DeepDream">full code here</a>.
                        If you have any doubts regarding this, feel free to contact me on my email or facebook. Thanks for sticking till the very end of blog :) Next week
                        we'll see Image Captioning.
                       </p>

                       <h2>References</h2>

                         <ul style="list-style-type:disc">
                           <li><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf">CS23n1 lecture on visualizing Convnets.</a></li>
                           <li><a href="https://www.youtube.com/watch?v=MrBzgvUNr4w&t=300s">Siraj Raval's video on Deep Dream.</a></li>
                           <li><a href="https://www.youtube.com/watch?v=ws-ZbiFV1Ms&t=257s">Hvass Laboratories tutorial on Deep Dream.</a></li>
                         </ul>




                                  <!-- Post Blockquote Start -->


                                   </div>
                                   <!-- Post Comment (Disqus) End -->



                                </div>
                             </div>

                         </div>
                       <!-- Footer Start -->
                       <div class="col-md-12 page-body margin-top-50 footer">
                          <footer>
                          <ul class="menu-link">
                            <li><a href="index.html">Home</a></li>
                            <li><a href="contact.html">Contact</a></li>
                            </ul>

                          <p>© Copyright 2016 DevBlog. All rights reserved</p>


						  <!-- UiPasta Credit Start -->
                          <div class="uipasta-credit">Design By <a href="http://www.uipasta.com" target="_blank">UiPasta</a></div>
                          <!-- UiPasta Credit End -->


                         </footer>
                       </div>
                       <!-- Footer End -->


                  </div>
                  <!-- Blog Post (Right Sidebar) End -->

            </div>
         </div>
      </div>



    <!-- Back to Top Start -->
    <a href="#" class="scroll-to-top"><i class="fa fa-long-arrow-up"></i></a>
    <!-- Back to Top End -->


    <!-- All Javascript Plugins  -->
    <script type="text/javascript" src="js/jquery.min.js"></script>
    <script type="text/javascript" src="js/plugin.js"></script>

    <!-- Main Javascript File  -->
    <script type="text/javascript" src="js/scripts.js"></script>

    <!-- Syntax Highlighter Javascript File  -->
    <script type="text/javascript" src="js/syntax/shCore.js"></script>
    <script type="text/javascript" src="js/syntax/shBrushCss.js"></script>
    <script type="text/javascript" src="js/syntax/shBrushJScript.js"></script>
    <script type="text/javascript" src="js/syntax/shBrushPerl.js"></script>
    <script type="text/javascript" src="js/syntax/shBrushPhp.js"></script>
    <script type="text/javascript" src="js/syntax/shBrushPlain.js"></script>
    <script type="text/javascript" src="js/syntax/shBrushPython.js"></script>
    <script type="text/javascript" src="js/syntax/shBrushRuby.js"></script>
    <script type="text/javascript" src="js/syntax/shBrushSql.js"></script>
    <script type="text/javascript" src="js/syntax/shBrushVb.js"></script>
    <script type="text/javascript" src="js/syntax/shBrushXml.js"></script>

	<!-- Syntax Highlighter Call Function -->
	<script type="text/javascript">
		SyntaxHighlighter.config.clipboardSwf = 'js/syntax/clipboard.swf';
		SyntaxHighlighter.all();
	</script>


   </body>
 </html>
